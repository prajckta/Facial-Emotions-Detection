{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LkGE43RNt4C"
      },
      "source": [
        "# Model Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsl55KMFNUlM",
        "outputId": "89872bee-792f-40a4-bb89-29df1bc725f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "EmotionDetection(\n",
              "  (conv1): Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4))\n",
              "  (pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv2): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "  (pool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv3): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv4): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (norm3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (fc): Sequential(\n",
              "    (0): Dropout(p=0.25, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Linear(in_features=4096, out_features=7, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "trans      = transforms.Compose([transforms.Resize((224,224)),\n",
        "                                 transforms.Grayscale(num_output_channels=1),\n",
        "                                 transforms.ToTensor()])\n",
        "#Define Model\n",
        "class EmotionDetection(nn.Module):\n",
        "    def __init__(self,num_classes):\n",
        "        super(EmotionDetection, self).__init__()\n",
        "        #Since we are converting image to gray scale use input channel as 1\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=11, stride=4)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        self.norm1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        nn.Dropout(0.25),\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 192, kernel_size=5, padding=2)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        self.norm2 = nn.BatchNorm2d(192)\n",
        "\n",
        "        nn.Dropout(0.25),\n",
        "\n",
        "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3, padding=1)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3, padding=1)\n",
        "\n",
        "        nn.Dropout(0.25)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        self.norm3 = nn.BatchNorm2d(256)\n",
        "\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "#         print(x.size())\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = F.relu(self.conv5(x))\n",
        "\n",
        "        x = self.pool3(x)\n",
        "        x = self.norm3(x)\n",
        "        x = self.avgpool(x)\n",
        "\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "net = EmotionDetection(7)\n",
        "net.load_state_dict(torch.load(\"/content/drive/MyDrive/Emotion Detector/Emotion_Detection.pt\"))\n",
        "net = net.to(device)\n",
        "net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OwR3bdKQqDf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCsVUM-YawU4"
      },
      "source": [
        "# Complete Function to Predict new image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S57t65bqcVjf",
        "outputId": "4c35546c-5334-4071-c376-037a692f10c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "< cv2.CascadeClassifier 0x7bff0177bc30>"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "os.chdir('/content/drive/MyDrive/Emotion Detector')\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "face_cascade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8n12z9vXf5K"
      },
      "outputs": [],
      "source": [
        "def predict(image, face_cascade):\n",
        "    image = np.array(image)\n",
        "    print(\"Processing Image .......\")\n",
        "    faces = face_cascade.detectMultiScale(image)\n",
        "\n",
        "    # Draw Bounding Box around Face\n",
        "    for x, y, w, h in faces:\n",
        "        roi_color = image[y:y+h, x:x+w]\n",
        "        cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 5)\n",
        "\n",
        "    if len(faces) == 0:\n",
        "         cv2.putText(image,'No Faces',(30,80),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
        "    else:\n",
        "        plt.imshow(image)\n",
        "\n",
        "        # Assuming 'trans' is defined somewhere in your code\n",
        "        trans_image = trans(Image.fromarray(roi_color, mode='RGB'))\n",
        "        trans_image = trans_image.unsqueeze(0)\n",
        "\n",
        "        # Predict image\n",
        "        with torch.no_grad():\n",
        "            trans_image = trans_image.to(device)\n",
        "            output = net(trans_image)\n",
        "            _, prediction = torch.max(output, 1)\n",
        "\n",
        "        print(output)\n",
        "        classes = ['angry', 'disgusted', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
        "        print(classes[prediction.item()])\n",
        "        plt.show()  # Show the final image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCe3BxPeiVeM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpFYbwWviZGO"
      },
      "source": [
        "- Try to add logic so that the rectangles with width proportional to iunput image are used. Ignore smaller rectangles\n",
        "- Add the text making the background of text as white\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q93jc-M5hWlo"
      },
      "outputs": [],
      "source": [
        "def predict_with_labels(image, face_cascade):\n",
        "    image = np.array(image)\n",
        "    print(\"Processing\")\n",
        "    gray_image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "    # plt.imshow(gray_image)\n",
        "    # plt.show()\n",
        "    faces = face_cascade.detectMultiScale(gray_image)\n",
        "\n",
        "    if len(faces) == 0:\n",
        "      cv2.putText(image, \"No Faces Detected\", (50,50), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,0,0), 2, cv2.LINE_AA)\n",
        "    else:\n",
        "      # Process each detected face\n",
        "      print(f\"number of faces{len(faces)}\")\n",
        "      for x, y, w, h in faces:\n",
        "          print(x,y,w,h)\n",
        "          if w>20 or h>20:\n",
        "            # Extract the region of interest (ROI) for each face\n",
        "            roi_color = image[y:y+h, x:x+w]\n",
        "\n",
        "            # Draw a rectangle around each face\n",
        "            cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 5)\n",
        "\n",
        "            #Transform image before passing to model\n",
        "            trans_image = trans(Image.fromarray(roi_color, mode='RGB'))\n",
        "            trans_image = trans_image.unsqueeze(0)\n",
        "\n",
        "            # Predict image for each face\n",
        "            with torch.no_grad():\n",
        "                trans_image = trans_image.to(device)\n",
        "                output = net(trans_image)\n",
        "                _, prediction = torch.max(output, 1)\n",
        "\n",
        "            classes = ['angry', 'disgusted', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
        "            label = classes[prediction.item()]\n",
        "\n",
        "            # Add class label with white background on top of each detected rectangle\n",
        "\n",
        "            FONT_SCALE = 2e-2  # Adjust for larger font size in all images\n",
        "            THICKNESS_SCALE = 1e-2  # Adjust for larger thickness in all images\n",
        "            font_scale = min(w, h) * FONT_SCALE\n",
        "            thickness = math.ceil(max(w, h) * THICKNESS_SCALE)\n",
        "\n",
        "            cv2.putText(image, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255,0,0), thickness, cv2.LINE_AA)\n",
        "\n",
        "            print(label)\n",
        "    plt.imshow(image)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVl-Lw7danlx",
        "outputId": "261acc3b-e62a-42da-df5f-8f8d0e74e189"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing\n",
            "number of faces10\n",
            "735 155 28 28\n",
            "neutral\n",
            "699 133 38 38\n",
            "sad\n",
            "353 139 35 35\n",
            "happy\n",
            "442 129 36 36\n",
            "happy\n",
            "536 156 33 33\n",
            "happy\n",
            "611 159 34 34\n",
            "happy\n",
            "800 138 41 41\n",
            "happy\n",
            "860 127 46 46\n",
            "happy\n",
            "194 159 47 47\n",
            "fearful\n",
            "193 314 61 61\n",
            "neutral\n"
          ]
        }
      ],
      "source": [
        "img = Image.open(\"/content/family3.jpg\")\n",
        "predict_with_labels(img,face_cascade)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtytEJ2ixDjv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
